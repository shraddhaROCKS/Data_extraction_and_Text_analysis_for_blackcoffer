{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1wBGD7HNgPPOa1AZFGb2SvNObfyXNF7NW","authorship_tag":"ABX9TyPdbXTKdQpT4MTB+Cje+eqO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R0_eScxTTgnE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706850275276,"user_tz":-330,"elapsed":37012,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}},"outputId":"81a3d320-c172-45f1-ab15-283a8af77d25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["import packages"],"metadata":{"id":"KdKeVi-rUGcK"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ez3136LCTr2e","executionInfo":{"status":"ok","timestamp":1706865959684,"user_tz":-330,"elapsed":5490,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}},"outputId":"02970178-2368-4525-8f3b-49534ac59349"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["**read the file**"],"metadata":{"id":"Se0cxfr7Ubll"}},{"cell_type":"code","source":["df= pd.read_excel(\"/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx\")"],"metadata":{"id":"sfBnmKFtUMGW","executionInfo":{"status":"ok","timestamp":1706865971616,"user_tz":-330,"elapsed":1454,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["***DATA EXTRACTION***"],"metadata":{"id":"xrTEdnjgUjhO"}},{"cell_type":"code","source":["for index, row in df.iterrows():\n","  url = row['URL']\n","  url_id = row['URL_ID']\n","\n","# make a request to url\n","header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}\n","response = requests.get(url,headers=header)\n"],"metadata":{"id":"IviLc-OTUuV3","executionInfo":{"status":"ok","timestamp":1706865979018,"user_tz":-330,"elapsed":2224,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#create a beautifulsoup object\n","soup = BeautifulSoup(response.content, 'html.parser')"],"metadata":{"id":"sEpRt1iEVJp8","executionInfo":{"status":"ok","timestamp":1706865980661,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#find heading\n","title = soup.find('h1').get_text()"],"metadata":{"id":"Rd1kYr4zVm3Z","executionInfo":{"status":"ok","timestamp":1706865983312,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#find text\n","text = \"\"\n","try:\n","    for p in soup.find_all('p'):\n","      text += p.get_text()\n","except:\n","      print(\"can't get text of {}\".format(url_id))\n"],"metadata":{"id":"JJAQpxqLboVk","executionInfo":{"status":"ok","timestamp":1706865983998,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"MmRIEo0kCQQh","executionInfo":{"status":"ok","timestamp":1706780336844,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}},"outputId":"c2cdcf9a-d32f-4489-e035-4d34fc20c61b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ML and AI-based insurance premium model to predict premium to be charged by the insurance companyAutomate the Data Management ProcessRealtime Kibana Dashboard for a financial tech firmData Management, ETL, and Data AutomationHow To Secure (SSL) Nginx with Let’s Encrypt on Ubuntu (Cloud VM, GCP, AWS, Azure, Linode) and Add DomainDeploy and view React app(Nextjs) on cloud VM such as GCP, AWS, Azure, LinodeDeploy Nodejs app on a cloud VM such as GCP, AWS, Azure, LinodeGrafana Dashboard – Oscar AwardsRising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in FutureInternet Demand’s Evolution, Communication Impact, and 2035’s Alternative PathwaysRise of Cybercrime and its Effect in upcoming FutureAI/ML and Predictive ModelingSolution for Contact Centre ProblemsHow to Setup Custom Domain for Google App Engine Application?Code Review ChecklistAs business close to help prevent transmission of COVID-19, financial concerns and job losses are one of the first human impacts of the virus;COVID-19 is in decline in China. There are now more new cases every day in Europe than there were in China at the epidemic’s peak and Italy has surpassed it as the country with the most deaths from the virus It took 67 days to reach the first 100,000 confirmed cases worldwide, 11 days for this to increase to 200,000and just four to reach 300,000 confirmed cases – a figure now exceeded.In recent weeks, we have seen the significant economic impact of the coronavirus on financial markets and vulnerable industries such as manufacturing, tourism, hospitality and travel.\\xa0Travel and tourism account for 10% of the global GDP and 50 million jobs are at risk worldwide.Global tourism, travel and hospitality companies closing down affects SMEs globally. This, in turn, affects many people, typically the least well-paid and those self-employed or working in informal environments in the gig economy or in part-time work with zero-hours contracts. Some governments have announced economic measures to safeguard jobs, guarantee wages and support the self-employed, but there is a lack of clarity in many countries about how these measures will be implemented and how people will manage a loss of income in the short-term.Behind these statistics lie the human costs of the pandemic, from the deaths of friends and family to the physical effects of infection and the mental trauma and fear faced by almost everyone. Not knowing how this pandemic will play out affects our economic, physical and mental well-being against a backdrop of a world that, for many, is increasingly anxious, unhappy and lonely.Fear of the unknown can often lead to feelings of panic, for example when people feel they are being denied life-saving protection or treatment or that they may run out of necessities, which can lead to panic buying.Psychological stress is often related to a sense of a lack of control in the face of uncertainty.In all cases, lack of information or the wrong information, either provided inadvertently or maliciously, can amplify the effects. There is a huge amount of misleading information circulating online about COVID-19, from fake medical information to speculation about government responses. People are susceptible to social media posts from an apparently trustworthy source, often referred to as an “Uncle with a Masters”-post, possibly amplified and spread by “copypasta” posts, which share information by copying and pasting and make each new post look like an original source, as opposed to posts that are “liked” or “shared” or “retweeted”.Sadly, criminals and hackers are also exploiting this situation and there has been a significant rise in Coronavirus-themed malicious websites, with more than 16,000 new coronavirus-related domains registered since January 2020. Hackers are selling malware and hacking tools through COVID-19 discount codes on the darknet,many of which are aimed at accessing corporate data from home-workers’ laptops, which may not be as secure as outside an office environment.Social distancing and lockdowns have also prompted altruistic behaviors, in part because of the sense that “we’re all in this together”. Many people report being bored or concerned about putting on weight;\\xa0others have discovered a slower pace of life and by not going out and socializing have found more time for family, others, and even their pets.The downside of self-isolation or social lockdown are symptoms of traumatic stress, confusion and anger, all of which are exacerbated by fear of infection, having limited access to supplies of necessities, inadequate information or the experience of economic loss or stigma. This stress and anxiety can lead to increased alcohol consumption, as well as an increase in domestic and family violence.In Jingzhou, a town near Wuhan in Hubei province, reports of domestic violence during the lockdown in February 2020 were more than triple the number reported in February 2019.Health measures must be the first priority for governments, business and society. It is important for businesses to show solidarity and work together to protect staff, local communities and customers, as well as keeping supply chains, manufacturing and logistics working.According to research, “my employer” is more trusted than the government or media. Daily updates on a company website with input from scientists and experts are recommended to counter politicized messages in the media and from governments. This is particularly true for large companies that have the capacity to do this.Messages about what businesses are doing for their employees and in their communities is also important. Some companies are helping schoolchildren from vulnerable families who can no longer get a school meal; others are providing public health messages about effective handwashing. Even CEOs can show they are working from home and self-isolating, while still being effective in their leadership.Following WHO advice, there is a need for the business community to move from general support to specific actions and focus on countries’ access to critical supplies, including a “Community Package of Critical Items” (a list of 46 items that all countries need). Of these items, 20 are either not available locally or available stocks are too limited. These missing items fall into four categories:The call for action is for more money, to work with manufacturers to create capacity and to organize purchasing so there is guaranteed access, especially for poorer countries with less resilient public health systems. The concept is to create a global security stockpile of supplies and equipment, an effort that needs:We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.Contact us: hello@blackcoffer.com© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#write title and text to the file\n","file_name = '/content/drive/MyDrive/Data Extraction Assignment/TitleText' + str(url_id) + '.txt'\n","with open(file_name, 'w') as file:\n","  file.write(title + '\\n' + text)"],"metadata":{"id":"EGis4SRtCRxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Directories\n","text_dir = \"/content/drive/MyDrive/Data Extraction Assignment/TitleText\"\n","stopwords_dir = \"/content/drive/MyDrive/20211030 Test Assignment/StopWords\"\n","sentment_dir = \"/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary\"\n","\n","# load all stop words from the stopwords directory and store in the set variable\n","stop_words = set()\n","for files in os.listdir(stopwords_dir):\n","  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n","    stop_words.update(set(f.read().splitlines()))\n"],"metadata":{"id":"SYyEB4GaK3np","executionInfo":{"status":"ok","timestamp":1706865988963,"user_tz":-330,"elapsed":1845,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# load all text files  from the  directory and store in a list(docs)\n","docs = []\n","for text_file in os.listdir(text_dir):\n","  with open(os.path.join(text_dir,text_file),'r') as f:\n","    text = f.read()\n","#tokenize the given text file\n","    words = word_tokenize(text)\n","# remove the stop words from the tokens\n","    filtered_text = [word for word in words if word.lower() not in stop_words]\n","# add each filtered tokens of each file into a list\n","    docs.append(filtered_text)"],"metadata":{"id":"IZexuwCrLyMh","executionInfo":{"status":"ok","timestamp":1706865989411,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# store positive, Negative words from the directory\n","pos=set()\n","neg=set()\n","\n","for files in os.listdir(sentment_dir):\n","  if files =='positive-words.txt':\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      pos.update(f.read().splitlines())\n","  else:\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      neg.update(f.read().splitlines())"],"metadata":{"id":"wxOVz1TOMBYg","executionInfo":{"status":"ok","timestamp":1706865993341,"user_tz":-330,"elapsed":969,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# now collect the positive  and negative words from each file\n","# calculate the scores from the positive and negative words\n","positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","#iterate through the list of docs\n","for i in range(len(docs)):\n","  positive_words.append([word for word in docs[i] if word.lower() in pos])\n","  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n","  positive_score.append(len(positive_words[i]))\n","  negative_score.append(len(Negative_words[i]))\n","  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))"],"metadata":{"id":"RFjREFKfMFjP","executionInfo":{"status":"ok","timestamp":1706865994712,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Average Sentence Length = the number of words / the number of sentences\n","# Percentage of Complex words = the number of complex words / the number of words\n","# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","avg_sentence_length = []\n","Percentage_of_Complex_words  =  []\n","Fog_Index = []\n","complex_word_count =  []\n","avg_syllable_word_count =[]\n","\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","def measure(file):\n","  with open(os.path.join(text_dir, file),'r') as f:\n","    text = f.read()\n","# remove punctuations\n","    text = re.sub(r'[^\\w\\s.]','',text)\n","# split the given text file into sentences\n","    sentences = text.split('.')\n","# total number of sentences in a file\n","    num_sentences = len(sentences)\n","# total words in the file\n","    words = [word  for word in text.split() if word.lower() not in stopwords ]\n","    num_words = len(words)\n","\n","# complex words having syllable count is greater than 2\n","# Complex words are words in the text that contain more than two syllables.\n","    complex_words = []\n","    for word in words:\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word > 2:\n","        complex_words.append(word)\n","\n","# Syllable Count Per Word\n","# We count the number of Syllables in each word of the text by counting the vowels present in each word.\n","#  We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n","    syllable_count = 0\n","    syllable_words =[]\n","    for word in words:\n","      if word.endswith('es'):\n","        word = word[:-2]\n","      elif word.endswith('ed'):\n","        word = word[:-2]\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word >= 1:\n","        syllable_words.append(word)\n","        syllable_count += syllable_count_word\n","\n","\n","    avg_sentence_len = num_words / num_sentences\n","    avg_syllable_word_count = syllable_count / len(syllable_words)\n","    Percent_Complex_words  =  len(complex_words) / num_words\n","    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n","\n","    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n","\n","# iterate through each file or doc\n","for file in os.listdir(text_dir):\n","  x,y,z,a,b = measure(file)\n","  avg_sentence_length.append(x)\n","  Percentage_of_Complex_words.append(y)\n","  Fog_Index.append(z)\n","  complex_word_count.append(a)\n","  avg_syllable_word_count.append(b)"],"metadata":{"id":"31RQnCWcMj5S","executionInfo":{"status":"ok","timestamp":1706865998212,"user_tz":-330,"elapsed":733,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n","# We count the total cleaned words present in the text by\n","# removing the stop words (using stopwords class of nltk package).\n","# removing any punctuations like ? ! , . from the word before counting.\n","\n","def cleaned_words(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    text = re.sub(r'[^\\w\\s]', '' , text)\n","    words = [word  for word in text.split() if word.lower() not in stopwords]\n","    length = sum(len(word) for word in words)\n","    average_word_length = length / len(words)\n","  return len(words),average_word_length\n","\n","word_count = []\n","average_word_length = []\n","for file in os.listdir(text_dir):\n","  x, y = cleaned_words(file)\n","  word_count.append(x)\n","  average_word_length.append(y)\n","\n","\n","# To calculate Personal Pronouns mentioned in the text, we use regex to find\n","# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n","#  so that the country name US is not included in the list.\n","def count_personal_pronouns(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    count = 0\n","    for pronoun in personal_pronouns:\n","      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n","  return count\n","\n","pp_count = []\n","for file in os.listdir(text_dir):\n","  x = count_personal_pronouns(file)\n","  pp_count.append(x)"],"metadata":{"id":"xp7vrFDTNr-Q","executionInfo":{"status":"ok","timestamp":1706866000055,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shraddha Choudhary","userId":"00312767324543122172"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["output_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","\n","# These are the required parameters\n","variables = [positive_score,\n","            negative_score,\n","            polarity_score,\n","            subjectivity_score,\n","            avg_sentence_length,\n","            Percentage_of_Complex_words,\n","            Fog_Index,\n","            avg_sentence_length,\n","            complex_word_count,\n","            word_count,\n","            avg_syllable_word_count,\n","            pp_count,\n","            average_word_length]\n","\n","\n","# write the values to the dataframe\n","if len(variables) == len(output_df.columns) - 2:\n","  for i, var in enumerate(variables):\n","    output_df.iloc[:,i+2] = var\n","\n","#now save the dataframe to the disk\n","output_df.t0_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n"],"metadata":{"id":"fAPilkO1KbJY"},"execution_count":null,"outputs":[]}]}